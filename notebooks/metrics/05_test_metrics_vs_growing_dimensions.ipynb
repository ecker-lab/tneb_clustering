{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f726249",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "from sklearn.metrics import davies_bouldin_score\n",
    "from sklearn.metrics import calinski_harabasz_score\n",
    "import warnings\n",
    "from tqdm import tqdm\n",
    "from sklearn.manifold import trustworthiness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eb91b3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# adopted from https://github.com/jqmviegas/jqm_cvi/blob/master/jqmcvi/base.py#L72\n",
    "\n",
    "def delta_fast(ck, cl, distances):\n",
    "    values = distances[np.where(ck)][:, np.where(cl)]\n",
    "    values = values[np.nonzero(values)]\n",
    "\n",
    "    return np.min(values)\n",
    "    \n",
    "def big_delta_fast(ci, distances):\n",
    "    values = distances[np.where(ci)][:, np.where(ci)]\n",
    "    #values = values[np.nonzero(values)]\n",
    "            \n",
    "    return np.max(values)\n",
    "\n",
    "def dunn_fast(points, labels):\n",
    "    \"\"\" Dunn index - FAST (using sklearn pairwise euclidean_distance function)\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    points : np.array\n",
    "        np.array([N, p]) of all points\n",
    "    labels: np.array\n",
    "        np.array([N]) labels of all points\n",
    "    \"\"\"\n",
    "    distances = euclidean_distances(points)\n",
    "    ks = np.sort(np.unique(labels))\n",
    "    \n",
    "    deltas = np.ones([len(ks), len(ks)])*1000000\n",
    "    big_deltas = np.zeros([len(ks), 1])\n",
    "    \n",
    "    l_range = list(range(0, len(ks)))\n",
    "    \n",
    "    for k in l_range:\n",
    "        for l in (l_range[0:k]+l_range[k+1:]):\n",
    "            deltas[k, l] = delta_fast((labels == ks[k]), (labels == ks[l]), distances)\n",
    "        \n",
    "        big_deltas[k] = big_delta_fast((labels == ks[k]), distances)\n",
    "\n",
    "    di = np.min(deltas)/np.max(big_deltas)\n",
    "    return di"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7186a98",
   "metadata": {},
   "source": [
    "## For uniform centers, 10 clusters,  with normal distr, std=0.3  I go through 2, 4, 8, 16, 32, 64, 128, 256, 512 dimesnions and check how the metrics behave\n",
    "\n",
    "For each dimension there are 10 datasets, so that we could everage the metric "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a382fee4",
   "metadata": {},
   "source": [
    "Dunn Index= $\\frac{\\min(\\text{inter-cluster distances})}{\\max(\\text{intra-cluster distances})} \\$\n",
    "\n",
    "DB = $\\frac{1}{n} \\sum_{i=1}^{n} \\max_{j\\neq i}\\left(\\frac{\\text{intra-cluster distance}(i) + \\text{intra-cluster distance}(j)}{\\text{inter-cluster distance}(i, j)}\\right)$\n",
    "\n",
    "CH = $\\frac{\\text{Between-Cluster Variance}}{\\text{Within-Cluster Variance}} \\times \\frac{N - k}{k - 1}$\n",
    "* Between-Cluster Variance is the variance between cluster means.\n",
    "* Within-Cluster Variance is the variance within clusters.\n",
    "* N is the total number of data points.\n",
    "* k is the number of clusters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d966dbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "std = 0.3\n",
    "n_samples = 1000\n",
    "n_centers = 10\n",
    "n_samples_around_c = 100\n",
    "\n",
    "datasets = {}\n",
    "for i in range(1, 10):\n",
    "    dim = 2 ** i\n",
    "    datasets[dim] = {}\n",
    "    datasets[dim]['embedings'] = []\n",
    "    datasets[dim]['labels'] = []\n",
    "    \n",
    "    for _ in range(10):\n",
    "        \n",
    "        cluster_centers = np.random.uniform(0, 1,size=(n_centers, dim))\n",
    "        latent_emb = []\n",
    "        labels = []\n",
    "\n",
    "        # create data\n",
    "        for ci, c in enumerate(cluster_centers):\n",
    "#           CAREFUL - check if you actually want to scale the variance!  \n",
    "            samples = np.random.normal(c, std * dim, size=(n_samples_around_c, dim))\n",
    "            latent_emb.append(samples)\n",
    "            labels.append(np.ones(len(samples))*ci)\n",
    "\n",
    "        latent_emb = np.array(latent_emb).reshape(n_centers*n_samples_around_c, -1)\n",
    "        labels = np.array(labels).reshape(n_centers*n_samples_around_c, -1)\n",
    "        datasets[dim]['embedings'].append(latent_emb)\n",
    "        datasets[dim]['labels'].append(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ec30ab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters = list(range(2, 20)) + list(range(20, 60, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0574ae87",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# THIS takes like 10 mins\n",
    "\n",
    "# Ignore all warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "results = {}\n",
    "# results['BIC'] = {} - this needs likelihood, which is not a part of k-means\n",
    "# results['AIC'] = {} - this needs likelihood, which is not a part of k-means\n",
    "results['dunn'] = {}\n",
    "results['silhouette'] = {}\n",
    "results['Davies–Bouldin'] = {}\n",
    "results['Calinski-Harabasz'] = {}\n",
    "\n",
    "for i in tqdm(range(1, 10)): \n",
    "    dim = 2 ** i\n",
    "    results['dunn'][dim] = {}\n",
    "    results['silhouette'][dim] = {}\n",
    "    results['Davies–Bouldin'][dim] = {}\n",
    "    results['Calinski-Harabasz'][dim] = {}\n",
    "    for c in tqdm(clusters):\n",
    "        results['dunn'][dim][c] = []\n",
    "        results['silhouette'][dim][c] = []\n",
    "        results['Davies–Bouldin'][dim][c] = []\n",
    "        results['Calinski-Harabasz'][dim][c] = []\n",
    "        \n",
    "        for idx in range(10):\n",
    "            loc = datasets[dim]['embedings'][idx]\n",
    "            loc_lab = datasets[dim]['labels'][idx]\n",
    "            \n",
    "            KMean = KMeans(n_clusters=c)\n",
    "            label = KMean.fit_predict(loc)\n",
    "            \n",
    "            results['dunn'][dim][c].append(\n",
    "                dunn_fast(loc, label)\n",
    "            )\n",
    "            results['silhouette'][dim][c].append(\n",
    "                silhouette_score(loc, label)\n",
    "            )\n",
    "            results['Davies–Bouldin'][dim][c].append(\n",
    "                davies_bouldin_score(loc, label)\n",
    "            )\n",
    "            results['Calinski-Harabasz'][dim][c].append(\n",
    "                calinski_harabasz_score(loc, label)\n",
    "            )\n",
    "           \n",
    "# Reset warning filters to default behavior\n",
    "warnings.filterwarnings(\"default\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5d470cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = clusters\n",
    "idxs = ['dunn', 'silhouette', 'Davies–Bouldin', 'Calinski-Harabasz']\n",
    "idxs_label = [r'dunn Index $\\uparrow$', r'silhouette Index $\\uparrow$', \n",
    "              r'Davies–Bouldin Index $\\downarrow$', r'Calinski-Harabasz Index $\\uparrow$']\n",
    "# Davies–Bouldin index (lower is better), Dunn index, silhouette score (higher is better)\n",
    "\n",
    "fig, axes = plt.subplots(nrows=2, ncols=2, figsize=(10, 8))\n",
    "for idx, ax in enumerate(axes.flatten()):\n",
    "    loc_key = idxs[idx]\n",
    "    for i in range(1, 10): \n",
    "        dim = 2 ** i\n",
    "        y = [np.mean(results[loc_key][dim][c]) for c in clusters]\n",
    "        errors = [np.sqrt(np.var(results[loc_key][dim][c])) for c in clusters]\n",
    "        ax.errorbar(x, y, yerr=errors, fmt='o-', label=str(dim), markersize=4, capsize=3)\n",
    "        ax.set_title(f'{idxs_label[idx]}')\n",
    "handles, labels = axes[0, 0].get_legend_handles_labels()\n",
    "fig.legend(handles, labels, loc='right')\n",
    "\n",
    "# Add global title\n",
    "fig.suptitle('Indexes comparison, std for 10 runs, 10 real clusters,\\ncolor codes dimensionality, k-means clustering', fontsize=14)\n",
    "\n",
    "# Add global axis label\n",
    "fig.text(0.5, 0.04, 'amount of clusters', ha='center', va='center', fontsize=14)\n",
    "fig.text(0.06, 0.5, 'metric value', ha='center', va='center', rotation='vertical', fontsize=14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "948cdc57",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=3, ncols=3, figsize=(10, 8))\n",
    "for i, ax in enumerate(axes.flatten()):\n",
    "    dim = 2 ** (i + 1)\n",
    "    \n",
    "    latent_emb = datasets[dim]['embedings'][0]\n",
    "    tsne_emb = TSNE(n_components=2, perplexity=30).fit_transform(latent_emb)\n",
    "    ax.scatter(tsne_emb[:, 0], tsne_emb[:, 1])\n",
    "    trust = round(trustworthiness(latent_emb, tsne_emb), 2)\n",
    "    ax.set_title(f'dim={dim},\\ntrustworthiness={trust}')\n",
    " \n",
    "fig.suptitle(f'TSNE from different dimensions, std={std}')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61c6860c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=3, ncols=3, figsize=(10, 8))\n",
    "for i, ax in enumerate(axes.flatten()):\n",
    "    dim = 2 ** (i + 1)\n",
    "    \n",
    "    latent_emb = datasets[dim]['embedings'][0]\n",
    "    tsne_emb = TSNE(n_components=2, perplexity=30).fit_transform(latent_emb)\n",
    "    ax.scatter(tsne_emb[:, 0], tsne_emb[:, 1], s=5)\n",
    "    trust = round(trustworthiness(latent_emb, tsne_emb), 2)\n",
    "    ax.set_title(f'dim={dim},\\ntrustworthiness={trust}')\n",
    " \n",
    "fig.suptitle(f'TSNE from different dimensions, std={std}')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31cc3e13",
   "metadata": {},
   "source": [
    "## Scale std with sqrt(dimensions) \n",
    "\n",
    "https://stats.stackexchange.com/questions/129885/why-does-increasing-the-sample-size-lower-the-sampling-variance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d246355",
   "metadata": {},
   "source": [
    "### Seems like more then sqrt(var) noise is needed in higher dimensions  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
