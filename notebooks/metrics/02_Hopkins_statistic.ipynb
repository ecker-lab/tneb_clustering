{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering of embeddings -> Cluster or continuum?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See implementation of Hopkins' statistics for Iris dataset\n",
    "\n",
    "https://github.com/prathmachowksey/Hopkins-Statistic-Clustering-Tendency/blob/master/Hopkins-Statistic-Clustering-Tendency.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Index for Clustering Tendency\n",
    "\n",
    "Brian Hopkins (1954) develops a method to determine the type of distribution for his plant data. \n",
    "\n",
    "The method calculation is as follows:\n",
    "\n",
    "- Calculate $\\sum P$ from $n$ observations, where $P$ is the distance from a random point (uniform distribution) to the nearest neighbor in $X$.\n",
    "- Calculate $\\sum I$ from $n$ observations, where $I$ is the distance from a  point chosen at random from $X$ to its nearest neighbor in $X$.\n",
    "- Calculate $A = \\sum P / \\sum I$\n",
    "- Calculate $x = A / (1+A)$\n",
    "\n",
    "---\n",
    "\n",
    "Lawson and Jurs (1990) describe the Hopkinsâ€™ statistic as well (same formula but written a bit easier, see derivation below):\n",
    "\n",
    "- Sample $n = 5 \\%$ of dataset points\n",
    "- $H = \\sum P / (\\sum P + \\sum I)$\n",
    "\n",
    "The null hypothesis is that the data is uniformly distributed and therefore is no organization in the data. Therefore, this statistic merely provides evidence that the data has more structure than uniformly distributed random numbers.\n",
    "\n",
    "This metric can be interpreted as follows:\n",
    "\n",
    "- if the data contains little structure, $H \\approx 0.5$\n",
    "- if the data is organized in tight clusters, $H \\approx 1.0$\n",
    "- $H > 0.75$ provides a 90% confidence that the data is more clustered than uniformly distributed random numbers, because of the shape of the Beta distribution.\n",
    "- $H \\approx 0.64$ could mislead researchers into concluding that more than one cluster is present"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## calculate Hopkins' statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to compute Hopkins' statistic for ndarray X\n",
    "def hopkins_statistic(X):\n",
    "\n",
    "    sample_size = int(X.shape[0] * 0.2) # 0.05 (5%) based on paper by Lawson and Jures\n",
    "    \n",
    "    # uniform random sample in the original data space\n",
    "    X_uniform_random_sample = np.random.uniform(X.min(axis=0), X.max(axis=0), (sample_size, X.shape[1]))\n",
    "    \n",
    "    # random sample of size sample_size from the original data X\n",
    "    random_indices = np.random.randint(0, X.shape[0], (sample_size,))\n",
    "    X_sample = X[random_indices]\n",
    "    \n",
    "    # initialize unsupervised learner for implementing neighbor searches\n",
    "    neigh = NearestNeighbors(n_neighbors=2)\n",
    "    nbrs = neigh.fit(X)\n",
    "    \n",
    "    # u_distances = nearest neighbour distances from uniform random sample\n",
    "    u_distances, u_indices = nbrs.kneighbors(X_uniform_random_sample, n_neighbors=2)\n",
    "    u_distances = u_distances[:, 0] # distance to the first (nearest) neighbour\n",
    "    \n",
    "    # w_distances = nearest neighbour distances from a sample of points from original data X\n",
    "    w_distances , w_indices = nbrs.kneighbors(X_sample, n_neighbors=2)\n",
    "    # distance to the second nearest neighbour (as the first neighbour will be the point itself, with distance = 0)\n",
    "    w_distances = w_distances[:, 1] \n",
    "    \n",
    "    u_sum = np.sum(u_distances)\n",
    "    w_sum = np.sum(w_distances)\n",
    "    \n",
    "    # compute and return Hopkins' statistic\n",
    "    H = u_sum / (u_sum + w_sum)\n",
    "    return H\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try on different data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sample points from uniform distribution with 32 dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 1000\n",
    "dim = 32\n",
    "\n",
    "n_centers = 10\n",
    "n_samples_around_c = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_emb = np.random.uniform(0,1,(n_samples, dim))\n",
    "latent_emb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustering = pd.DataFrame(TSNE(n_components=2, perplexity=30).fit_transform(latent_emb), columns=['x', 'y'])\n",
    "sns.scatterplot(data=clustering, x='x', y='y')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Hopkins statistic: {hopkins_statistic(latent_emb)}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hopkins' statistic is 0.5 which indicates uniform distribution which is correct."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## std = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_centers = np.random.uniform(0,1,size=(n_centers, dim))\n",
    "latent_emb = []\n",
    "labels = []\n",
    "\n",
    "# create data\n",
    "for ci, c in enumerate(cluster_centers):\n",
    "    samples = np.random.normal(c, 0.01, size=(n_samples_around_c, dim))\n",
    "    latent_emb.append(samples)\n",
    "    labels.append(np.ones(len(samples))*ci)\n",
    "\n",
    "latent_emb = np.array(latent_emb).reshape(n_centers*n_samples_around_c, -1)\n",
    "labels = np.array(labels).reshape(n_centers*n_samples_around_c, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustering = pd.DataFrame(TSNE(n_components=2, perplexity=30).fit_transform(latent_emb), columns=['x', 'y'])\n",
    "sns.scatterplot(data=clustering, x='x', y='y')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Hopkins statistic: {hopkins_statistic(latent_emb)}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hopkins' statistic is close to 1 which is exactly as expected of the metric if the data is strongly clustered as it is."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## std = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_centers = np.random.uniform(0,1,size=(n_centers, dim))\n",
    "latent_emb = []\n",
    "labels = []\n",
    "\n",
    "# create data\n",
    "for ci, c in enumerate(cluster_centers):\n",
    "    samples = np.random.normal(c, 0.5, size=(n_samples_around_c, dim))\n",
    "    latent_emb.append(samples)\n",
    "    labels.append(np.ones(len(samples))*ci)\n",
    "\n",
    "latent_emb = np.array(latent_emb).reshape(n_centers*n_samples_around_c, -1)\n",
    "labels = np.array(labels).reshape(n_centers*n_samples_around_c, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustering = pd.DataFrame(TSNE(n_components=2, perplexity=30).fit_transform(latent_emb), columns=['x', 'y'])\n",
    "sns.scatterplot(data=clustering, x='x', y='y')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Hopkins statistic: {hopkins_statistic(latent_emb)}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I would have expected a higher value of Hopkins' statistic here since the data is visibly clustered and a value of 0.64 does not necessarily mean clusterable data according to Lawson 1990."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## std = 0.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_centers = np.random.uniform(0,1,size=(n_centers, dim))\n",
    "latent_emb = []\n",
    "labels = []\n",
    "\n",
    "# create data\n",
    "for ci, c in enumerate(cluster_centers):\n",
    "    samples = np.random.normal(c, 0.7, size=(n_samples_around_c, dim))\n",
    "    latent_emb.append(samples)\n",
    "    labels.append(np.ones(len(samples))*ci)\n",
    "\n",
    "latent_emb = np.array(latent_emb).reshape(n_centers*n_samples_around_c, -1)\n",
    "labels = np.array(labels).reshape(n_centers*n_samples_around_c, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustering = pd.DataFrame(TSNE(n_components=2, perplexity=30).fit_transform(latent_emb), columns=['x', 'y'])\n",
    "sns.scatterplot(data=clustering, x='x', y='y')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Hopkins statistic: {hopkins_statistic(latent_emb)}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The value does not drop compared to clustering before although it is visibly less clustered."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## std = 0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_centers = np.random.uniform(0,1,size=(n_centers, dim))\n",
    "latent_emb = []\n",
    "labels = []\n",
    "\n",
    "# create data\n",
    "for ci, c in enumerate(cluster_centers):\n",
    "    samples = np.random.normal(c, 0.8, size=(n_samples_around_c, dim))\n",
    "    latent_emb.append(samples)\n",
    "    labels.append(np.ones(len(samples))*ci)\n",
    "\n",
    "latent_emb = np.array(latent_emb).reshape(n_centers*n_samples_around_c, -1)\n",
    "labels = np.array(labels).reshape(n_centers*n_samples_around_c, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustering = pd.DataFrame(TSNE(n_components=2, perplexity=30).fit_transform(latent_emb), columns=['x', 'y'])\n",
    "sns.scatterplot(data=clustering, x='x', y='y')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Hopkins statistic: {hopkins_statistic(latent_emb)}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## std = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_centers = np.random.uniform(0,1,size=(n_centers, dim))\n",
    "latent_emb = []\n",
    "labels = []\n",
    "\n",
    "# create data\n",
    "for ci, c in enumerate(cluster_centers):\n",
    "    samples = np.random.normal(c, 1.0, size=(n_samples_around_c, dim))\n",
    "    latent_emb.append(samples)\n",
    "    labels.append(np.ones(len(samples))*ci)\n",
    "\n",
    "latent_emb = np.array(latent_emb).reshape(n_centers*n_samples_around_c, -1)\n",
    "labels = np.array(labels).reshape(n_centers*n_samples_around_c, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustering = pd.DataFrame(TSNE(n_components=2, perplexity=30).fit_transform(latent_emb), columns=['x', 'y'])\n",
    "sns.scatterplot(data=clustering, x='x', y='y')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Hopkins statistic: {hopkins_statistic(latent_emb)}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## std = 3.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_centers = np.random.uniform(0,1,size=(n_centers, dim))\n",
    "latent_emb = []\n",
    "labels = []\n",
    "\n",
    "# create data\n",
    "for ci, c in enumerate(cluster_centers):\n",
    "    samples = np.random.normal(c, 3.0, size=(n_samples_around_c, dim))\n",
    "    latent_emb.append(samples)\n",
    "    labels.append(np.ones(len(samples))*ci)\n",
    "\n",
    "latent_emb = np.array(latent_emb).reshape(n_centers*n_samples_around_c, -1)\n",
    "labels = np.array(labels).reshape(n_centers*n_samples_around_c, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustering = pd.DataFrame(TSNE(n_components=2, perplexity=30).fit_transform(latent_emb), columns=['x', 'y'])\n",
    "sns.scatterplot(data=clustering, x='x', y='y')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Hopkins statistic: {hopkins_statistic(latent_emb)}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One normal distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_emb = np.random.normal(0, 1, size=(n_samples, dim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustering = pd.DataFrame(TSNE(n_components=2, perplexity=30).fit_transform(latent_emb), columns=['x', 'y'])\n",
    "sns.scatterplot(data=clustering, x='x', y='y')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Hopkins statistic: {hopkins_statistic(latent_emb)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.6 ('siren2')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "050244effc7d5c5cc73ae5a10f99a6a5eb5b067e7f4d1964b3c9e69c9fb1e9cd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
