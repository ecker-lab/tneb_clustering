{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ff6aff1c-bac3-4c75-874b-563bb2a2106f",
   "metadata": {},
   "source": [
    "# Density-Connected Datasets\n",
    "\n",
    "Generation scripts for DENSIRED scripts.\n",
    "\n",
    "This is the script that we used to generate all densired-datasets which are stored in densired.npz and densired_soft.npz in the datasets folder.\n",
    "\n",
    "Since densired requires `numpy>=2.0` while other packages rely on `numpy<2.0` one needs a dedicated environment with the package `densired` installed.\n",
    "\n",
    "In order to make sure that the dataset is not linearly separable one additionally requires `scikit-learn` and `torch`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-17T14:38:36.943424Z",
     "start_time": "2024-09-17T14:38:36.925765Z"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e81c4c97464b09f7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-17T14:30:27.412592Z",
     "start_time": "2024-09-17T14:30:21.343604Z"
    }
   },
   "outputs": [],
   "source": [
    "import densired\n",
    "import numpy as np\n",
    "import sklearn.preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2f31153-e4d9-4fa9-ba36-db0d532e679f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# note that the data was generated without fixing the seed. \n",
    "# So the created dataset will look slightly different.\n",
    "\n",
    "funky_shapes = dict()\n",
    "for dim in [8,16,32,64]:\n",
    "    skeleton = densired.datagen.densityDataGen(\n",
    "        dim=dim,\n",
    "        radius=5,\n",
    "        clunum=6,\n",
    "        core_num=200,\n",
    "        min_dist=0.7,\n",
    "        dens_factors=True,\n",
    "        step_spread=0.3,\n",
    "        # ratio_noise=0.1,\n",
    "        ratio_con=0.01,\n",
    "        seed=42\n",
    "    )\n",
    "    points = skeleton.generate_data(data_num=10000)\n",
    "\n",
    "    # normalize the data\n",
    "    X = points[:, :-1]\n",
    "    y = points[:, -1]\n",
    "    X = sklearn.preprocessingStandardScaler().fit_transform(X)\n",
    "    funky_shapes[dim] = X,y\n",
    "        \n",
    "with open('densired.npz', 'wb') as f:\n",
    "    np.savez(f, d8=funky_shapes[8], d16=funky_shapes[16], d32=funky_shapes[32], d64=funky_shapes[64])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "112c0c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# note that the data was generated without fixing the seed. \n",
    "# So the created dataset will look slightly different.\n",
    "\n",
    "funky_shapes = dict()\n",
    "for dim in [8,16,32,64]:\n",
    "    skeleton = densired.datagen.densityDataGen(\n",
    "        dim=dim,\n",
    "        radius=5,\n",
    "        clunum=6,\n",
    "        core_num=200,\n",
    "        min_dist=1.2,\n",
    "        dens_factors=True,\n",
    "        step_spread=0.3,\n",
    "        # ratio_noise=0.1,\n",
    "        ratio_con=0.01,\n",
    "        distribution=\"studentt\",\n",
    "        con_distribution=4,\n",
    "        seed=42\n",
    "    )\n",
    "    points = skeleton.generate_data(data_num=10000)\n",
    "\n",
    "    # normalize the data\n",
    "    X = points[:, :-1]\n",
    "    y = points[:, -1]\n",
    "    X = sklearn.preprocessingStandardScaler().fit_transform(X)\n",
    "    funky_shapes[dim] = X,y\n",
    "        \n",
    "with open('densired_soft.npz', 'wb') as f:\n",
    "    np.savez(f, d8=funky_shapes[8], d16=funky_shapes[16], d32=funky_shapes[32], d64=funky_shapes[64])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36410f2c",
   "metadata": {},
   "source": [
    "## Additional code to check whether the created dataset is linearly separable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f037de3a5b3c6de",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-17T14:27:03.911846Z",
     "start_time": "2024-09-17T14:27:03.886593Z"
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcorc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# constants\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import sklearn.model_selection\n",
    "\n",
    "# constants\n",
    "noise = True\n",
    "dim = data.shape[1]-1\n",
    "classes = int(np.max(data[:,-1]))+1 \n",
    "if noise:\n",
    "    classes += 1 # for the noise class\n",
    "\n",
    "\n",
    "# data loading\n",
    "X = torch.tensor(data[:,:dim], dtype=torch.float32)\n",
    "Y = torch.tensor(data[:,-1], dtype=torch.long) \n",
    "if noise:\n",
    "    Y += 1# shift to avoid -1 for the noise points\n",
    "X_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(X, Y, test_size=0.2, random_state=42)\n",
    "train_dataset = torch.utils.data.TensorDataset(X_train, y_train)\n",
    "test_dataset = torch.utils.data.TensorDataset(X_test, y_test)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "# model = torch.nn.Sequential(\n",
    "#     torch.nn.Linear(dim, 64),\n",
    "#     torch.nn.ReLU(),\n",
    "#     torch.nn.Linear(64, 64),\n",
    "#     torch.nn.ReLU(),\n",
    "#     torch.nn.Linear(64, classes),\n",
    "# )\n",
    "\n",
    "model = torch.nn.Linear(dim, classes)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b44e5c400d8a85fa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-17T14:27:04.707231Z",
     "start_time": "2024-09-17T14:27:04.679760Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100, Loss: 105.89410564804078\n",
      "Epoch 2/100, Loss: 11.284251786231994\n",
      "Epoch 3/100, Loss: 5.509677130937576\n",
      "Epoch 4/100, Loss: 3.7453300144672395\n",
      "Epoch 5/100, Loss: 2.8857780869007112\n",
      "Epoch 6/100, Loss: 2.3912080216407774\n",
      "Epoch 7/100, Loss: 2.042907996356487\n",
      "Epoch 8/100, Loss: 1.781401772737503\n",
      "Epoch 9/100, Loss: 1.5620567571222783\n",
      "Epoch 10/100, Loss: 1.3821382905095816\n",
      "Epoch 11/100, Loss: 1.2176095722615718\n",
      "Epoch 12/100, Loss: 1.0806215129494667\n",
      "Epoch 13/100, Loss: 0.9619653740674258\n",
      "Epoch 14/100, Loss: 0.8607465152740479\n",
      "Epoch 15/100, Loss: 0.7765681564062834\n",
      "Epoch 16/100, Loss: 0.6978154877275229\n",
      "Epoch 17/100, Loss: 0.6373611835241317\n",
      "Epoch 18/100, Loss: 0.5815453063994646\n",
      "Epoch 19/100, Loss: 0.5354588404297829\n",
      "Epoch 20/100, Loss: 0.5025512109994889\n",
      "Epoch 21/100, Loss: 0.4624458074569702\n",
      "Epoch 22/100, Loss: 0.4288310478180647\n",
      "Epoch 23/100, Loss: 0.4024270610436797\n",
      "Epoch 24/100, Loss: 0.38066316889971497\n",
      "Epoch 25/100, Loss: 0.35783379547297955\n",
      "Epoch 26/100, Loss: 0.33913001462072134\n",
      "Epoch 27/100, Loss: 0.32550784067064525\n",
      "Epoch 28/100, Loss: 0.3017336070090532\n",
      "Epoch 29/100, Loss: 0.29181125430390237\n",
      "Epoch 30/100, Loss: 0.27332165389508006\n",
      "Epoch 31/100, Loss: 0.265006031781435\n",
      "Epoch 32/100, Loss: 0.2557816398479044\n",
      "Epoch 33/100, Loss: 0.24743896656110884\n",
      "Epoch 34/100, Loss: 0.23264669357985257\n",
      "Epoch 35/100, Loss: 0.2224736868813634\n",
      "Epoch 36/100, Loss: 0.21256608948111533\n",
      "Epoch 37/100, Loss: 0.21030792012810706\n",
      "Epoch 38/100, Loss: 0.20141567525826395\n",
      "Epoch 39/100, Loss: 0.19471931024827063\n",
      "Epoch 40/100, Loss: 0.19293785469513386\n",
      "Epoch 41/100, Loss: 0.18213855397887527\n",
      "Epoch 42/100, Loss: 0.177533439286286\n",
      "Epoch 43/100, Loss: 0.1789511032048613\n",
      "Epoch 44/100, Loss: 0.16423897013813257\n",
      "Epoch 45/100, Loss: 0.16966428259480745\n",
      "Epoch 46/100, Loss: 0.1587349800495431\n",
      "Epoch 47/100, Loss: 0.15609947690414264\n",
      "Epoch 48/100, Loss: 0.15400758575089277\n",
      "Epoch 49/100, Loss: 0.1460450757164508\n",
      "Epoch 50/100, Loss: 0.1475907033770345\n",
      "Epoch 51/100, Loss: 0.1430070444494486\n",
      "Epoch 52/100, Loss: 0.1439824762539938\n",
      "Epoch 53/100, Loss: 0.14251570029277355\n",
      "Epoch 54/100, Loss: 0.13660688424762338\n",
      "Epoch 55/100, Loss: 0.13166816550772636\n",
      "Epoch 56/100, Loss: 0.13240521031804384\n",
      "Epoch 57/100, Loss: 0.1272000202848576\n",
      "Epoch 58/100, Loss: 0.1294564526011236\n",
      "Epoch 59/100, Loss: 0.12741300176177173\n",
      "Epoch 60/100, Loss: 0.12513403580198065\n",
      "Epoch 61/100, Loss: 0.11907497603725642\n",
      "Epoch 62/100, Loss: 0.12126604376127943\n",
      "Epoch 63/100, Loss: 0.12159127619862556\n",
      "Epoch 64/100, Loss: 0.1185520313581219\n",
      "Epoch 65/100, Loss: 0.12226715899986448\n",
      "Epoch 66/100, Loss: 0.11801834718231112\n",
      "Epoch 67/100, Loss: 0.11723617846425623\n",
      "Epoch 68/100, Loss: 0.11730530068464577\n",
      "Epoch 69/100, Loss: 0.11598974649375304\n",
      "Epoch 70/100, Loss: 0.11690935918770265\n",
      "Epoch 71/100, Loss: 0.11421760170906782\n",
      "Epoch 72/100, Loss: 0.11303039010684006\n",
      "Epoch 73/100, Loss: 0.11185797507537064\n",
      "Epoch 74/100, Loss: 0.10907555910851806\n",
      "Epoch 75/100, Loss: 0.1070794724826701\n",
      "Epoch 76/100, Loss: 0.11141120592714288\n",
      "Epoch 77/100, Loss: 0.11163358538574539\n",
      "Epoch 78/100, Loss: 0.10549188143550418\n",
      "Epoch 79/100, Loss: 0.10360745032411069\n",
      "Epoch 80/100, Loss: 0.10213479462140822\n",
      "Epoch 81/100, Loss: 0.10490358945957269\n",
      "Epoch 82/100, Loss: 0.10448559202911566\n",
      "Epoch 83/100, Loss: 0.10500831661606208\n",
      "Epoch 84/100, Loss: 0.10333285758824787\n",
      "Epoch 85/100, Loss: 0.10841139137448044\n",
      "Epoch 86/100, Loss: 0.09842418270371854\n",
      "Epoch 87/100, Loss: 0.10032514811924192\n",
      "Epoch 88/100, Loss: 0.09910309645836242\n",
      "Epoch 89/100, Loss: 0.10254773939438747\n",
      "Epoch 90/100, Loss: 0.10092964173591464\n",
      "Epoch 91/100, Loss: 0.0988419962071348\n",
      "Epoch 92/100, Loss: 0.09332842678477755\n",
      "Epoch 93/100, Loss: 0.10458762176765594\n",
      "Epoch 94/100, Loss: 0.09782353499089368\n",
      "Epoch 95/100, Loss: 0.09567066183139104\n",
      "Epoch 96/100, Loss: 0.09896233720047166\n",
      "Epoch 97/100, Loss: 0.09565256404306274\n",
      "Epoch 98/100, Loss: 0.10203329757528264\n",
      "Epoch 99/100, Loss: 0.09745272521494189\n",
      "Epoch 100/100, Loss: 0.09747563902358525\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "# model training\n",
    "num_epochs = 100\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_loss = 0.0\n",
    "    for x,y in train_loader:\n",
    "        y_pred = model(x)\n",
    "        loss = torch.nn.functional.cross_entropy(y_pred, y)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item() * x.size(0)\n",
    "    avg_epoch_loss = epoch_loss / len(train_loader)\n",
    "    print(f'Epoch {epoch+1}/{num_epochs}, Loss: {avg_epoch_loss}')\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d6beb831cc6ffa8c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-17T14:27:05.283145Z",
     "start_time": "2024-09-17T14:27:05.258164Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.999\n"
     ]
    }
   ],
   "source": [
    "# check accuracy\n",
    "model.eval()\n",
    "total = 0\n",
    "correct = 0\n",
    "for x,y in test_loader:\n",
    "    y_pred = model(x)\n",
    "    _, y_pred = torch.max(y_pred, 1)\n",
    "    total += y.size(0)\n",
    "    correct += (y_pred == y).sum().item()\n",
    "accuracy = correct / total\n",
    "print(f'Accuracy: {accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "445bdb85ed5b53c6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-17T14:26:03.753198Z",
     "start_time": "2024-09-17T14:26:03.516216Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mspatial\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistance\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m cdist\n\u001b[0;32m----> 2\u001b[0m cdist(X_train, X_test, metric\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124meuclidean\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'X_train' is not defined"
     ]
    }
   ],
   "source": [
    "from scipy.spatial.distance import cdist\n",
    "cdist(X_train, X_test, metric='euclidean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28f00ca306422712",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "continuum",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
